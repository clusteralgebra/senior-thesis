\documentclass{puthesis-UG}
\usepackage{style}
%%%%%%%%IMPORTANT INFORMATION%%%%%%%%%%%%%%%%%%%%
%Using puthesis-UG.cls includes an Honor Code declaration. By using this file you are 
%declaring that this paper is your own work in accordance with University retgulations.
%
%The file puthesis-UG.cls must be downloaded from the math department website and 
%saved in the same directory as this file.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{Alan Yan}
\adviser{Professor June E. Huh}
\title{Log-concavity in Combinatorics}
\abstract{(write something flowery)}
\acknowledgements{I would like to thank}
\date{today}


% Things I want to talk about:

% 1. Combinatorial Atlas
% 2. Lorentzian Polynomials
% 3. Alexandrov-Fenchel Inequality
% 4. 
%


\begin{document}
 
\chapter{Conventions and Notation}

\chapter{Introduction}

\chapter{Combinatorial Structures}

\section{Partially Ordered Sets}

\section{Matroids}

Our main reference for matroid theory is \cite{10.5555/1197093}. 

\begin{defn}
	A \textbf{matroid} is an ordered pair $M = (E, \mcI)$ consisting of a finite set $E$ and a collection of subsets $\mathcal{I} \subseteq 2^E$ which satisfy the following three properties:
	\begin{enumerate}
		\item[(\textbf{I1})] $\emptyset \in \mathcal{I}$.
		\item[(\textbf{I2})] If $X \subseteq Y$ and $Y \in \mathcal{I}$, then $X \in \mathcal{I}$. 
		\item[(\textbf{I3})] If $X, Y \in \mathcal{I}$ and $|X| > |Y|$, then there exists some element $e \in X \backslash Y$ such that $Y \cup \{e\} \in \mathcal{I}$. 
	\end{enumerate}
	The set $E$ is called the ground set of the matroid and the collection of subsets $\mathcal{I}$ are called independent sets. 
\end{defn}

Given a matroid $M = (E, \mathcal{I})$, we call a subset $X \subseteq E$ a \textbf{dependent set} if and only if $X \notin \mathcal{I}$. We define a basis $B \in \mathcal{I}$ to be a maximal independent set. 

\begin{prop}
	Let $M = (E, \mathcal{I})$ be a matroid and $B_1, B_2$ be bases. Then $|B_1| = |B_2|$. 
\end{prop}
\begin{proof}
	If $|B_1| < |B_2|$, then by (I3), there exists some element $e \in B_2 \backslash B_1$ satisfying $B_1 \cup \{e\} \in \mathcal{I}$. But $|B_1 \cup \{e\}| = |B_1| + 1 > |B_1|$ which contradicts the maximality of $B_1$. 
\end{proof}

\begin{defn}
	For every matroid $M = (E, \mcI)$, we define its rank function $\rank_M : 2^E \to \NN$ to be equal to
	\[
		\rank_M(X) := \max \{|I| : I \in \mathcal{I}, I \subseteq X\}.
	\]
	The rank function satisfies 
\end{defn}

\begin{itemize}
	\item parallel classes, loops
	\item simple matroids
	\item contraction
	\item deletion
	\item flats
\end{itemize}

\section{Convex Bodies}

In this section, we review the notions of convexity and convex bodies. Our main reference for the theory of convex bodies is (cite).

\begin{defn}
	A \textbf{convex body} is a compact, convex subset of $\RR^n$.
\end{defn}
\subsection{Mixed Volumes}

\chapter{Mechanisms for Log-concavity}

\section{Alexandrov's Inequality for Mixed Discriminants}

\section{Alexandrov-Fenchel Inequality}

\section{Lorentzian Polynomials}

\section{Combinatorial Atlas}

\chapter{Log-concavity Results for Posets and Matroids}

\section{Stanley's Inequality}

\section{Kahn-Saks Inequality}

\section{Stanley's Matroid Inequality}


\chapter{Hard Lefschetz Property and Hodge-Riemann Relations}

\section{Chow Ring of Homogeneous Polynomials}

In this section, we study a cohomology ring studied in (CITE Murai). For every homogeneous polynomial $f \in \RR[x_1, \ldots, x_n]$ we can associate a graded ring which is a proxy for cohomology (measuring socles??? Think about this later)

\begin{defn}
	Let $f \in \RR[x_1, \ldots, x_n]$ be a homomgeneous polynoimal and let $S := \RR[\partial_1, \ldots, \partial_n]$ be the polynomial ring of differentials where $\partial_i := \partial_{x_i}$. We define the ring 
	\[
		A_f^\bullet := S / \Ann_S (f).
	\]
	which we call the Chow (???) ring. 
\end{defn}

We first prove that the Chow ring is a graded $\RR$-algebra. To do this, we first prove Lemma (REFERENCE)

\begin{lem} \label{homogeneous-parts}
	Let $\xi \in \RR[\partial_1, \ldots, \partial_n]$ and $f \in \RR[x_1, \ldots, x_n]$ be a homogeneous polynomial. We can decompose $\xi = \xi_0 + \xi_1 + \ldots$ into its homogeneous parts. If $\xi (f) = 0$, then $\xi_d (f) = 0$ for all $d \geq 0$. 
\end{lem}

\begin{proof}
	Let $d = \deg (f)$. If $\xi_i (f) \neq 0$, then $i \leq d$ and $\xi_i(f)$ is a homomgeneous polynomial of degree $d-i$. Thus, 
	\[
		\xi (f) = \xi_0 (f) + \xi_1(f) + \ldots
	\]
	will be the homomgeneous decomposition of the polynomial $\xi(f)$. Since this is equal to $0$, all components of the decomposition are equal to zero. This proves the proposition. 
\end{proof}

\begin{prop}
	The Chow ring $A_f^\bullet$ is a graded $\RR$-algebra where $A_f^k$ consists of the forms of degree $k$. 
\end{prop}

\begin{proof}
	Let us define $A_f^k$ as in the statement of the lemma. Let $d = \deg (f)$ be the degree of the homomgeneous polynomial. Whenver $k > d$, the ring $A_f^k$ is clearly trivial. From Lemma~\ref{homogeneous-parts}, we have the direct sum decomposition 
	\[
		A_f^\bullet = \bigoplus_{k = 0}^d A_f^k.
	\]
	It is also clear that multiplication induces maps $A_f^r \times A_f^s \to A_f^{r+s}$ for all $r, s \geq 0$.
\end{proof}

The following result (Proposition (CITE)) is well-known and it follows from Theorem 2.1 in CITE(Maeno, Watanabe, Lefschetz elements of ARtinian Gorenstein algebras and Hessians of homogeneous polynomials)
\begin{prop} \label{chow-ring-is-a-PD-algebra}
	Let $f$ be a homogeneous polynoimal of degree $d$. Then, the ring $A_f^\bullet$ is a Poincare-Duality algebra. That is, the ring satisfies the following two properties:
	\begin{enumerate}[label = (\alph*)]
		\item $A_f^d \simeq A_f^0 \simeq \RR$;

		\item The pairing induced by multiplication $A_f^{d-k} \times A_f^k \to A_f^d \simeq \RR$ is non-degenerate for all $0 \leq k \leq d$. 
	\end{enumerate}
\end{prop}

We first make a small remark on non-degeneracy to make it clear what we mean when we say a pairing is non-degenerate. 
\begin{lem} \label{non-degeneracy-definition}
	Let $B : V \times W \to k$ be a bilinear pairing between two finite-dimensional $k$-vector spaces $V$ and $W$. Then, any two of the following three conditions imply the third. 
		\begin{enumerate}[label = (\roman*)]
			\item The map $B_V : V \to W^*$ defined by $v \mapsto B(v, \cdot)$ has trivial kernel.
			\item The map $B_W : W \to V^*$ defined by $w \mapsto B(\cdot, w)$ has trivial kernel. 
			\item $\dim V = \dim W$. 
		\end{enumerate}
	\end{lem}

	\begin{proof}
		Condition (i) implies $\dim V \leq \dim W$ and Condition (ii) implies $\dim W \leq \dim V$. Thus (i) and (ii) both imply (iii). Now, suppose that (i) and (iii) are true. Then $B_V$ is an isomorphism between $V$ and $W*$ [3.69 in Axler (CITE???)]. Let $v_1, \ldots, v_n$ be a basis for $V$. Then $B_V(v_1), \ldots, B_V(v_n)$ is a basis of $W^*$. Let $w_1, \ldots, w_n$ be the dual basis in $W$ with respect to this basis of $W^*$. Suppose that $\sum \lambda_i w_i \in \ker B_W$. Then for all $v \in V$, we have 
		\[
			\sum_{i = 1}^n \lambda_i B_V(v)(w) = B \left ( v, \sum_{i = 1}^n \lambda_i w_i \right ) = 0. 
		\]
		By letting $v = v_1, \ldots, v_n$, we get $\lambda_i = 0$ for all $i$. 
	\end{proof}

We say a pairing between finite-dimensional vector spaces is non-degenerate whenever all three conditions in Lemma~\ref{non-degeneracy-definition} hold. As a corollary, we have the following result.

\begin{cor} \label{same-dimensions}
	Let $f$ be a homogeneous polynomial of degree $d \geq 2$ and let $k$, $0 \leq k \leq d$, by a non-negative integer. Then $\dim_\RR A_f^k = \dim_\RR A_f^{d-k}$. 
\end{cor}

In Proposition(CITE), we can define $\deg_f : A_f^d \to \RR$ to be the isomorphism defined by evaluation at $f$. That is, for any $\xi \in A_f^d$, we can define $\deg_f (\xi) := \xi(f)$ where $\xi$ acts on $f$ by differentiation. FOllowing (CITE HODGE THEORY OF COMBINATORIAL GEOMETRIES), we formulate the following definition

\begin{defn}
	Let $f$ be a homogeneous polynomial of degree $d$ and let $k \leq d/2$ be a non-negative integer. For an element $l \in A_f^1$, we define the following notions:
	\begin{enumerate}[label = (\alph*)]
		\item The \textbf{Lefschetz operator} on $A_f^k$ associated to $l$ is the map $L_l^k : A_f^k \to A_f^{d-k}$ defined by $\xi \mapsto l^{d-2k} \cdot \xi$. 

		\item The \textbf{Hodge-Riemann form} on $A_f^k$ associated to $l$ is the bilinear form $Q_l^k : A_f^k \times A_f^k \to \RR$ defined by $Q_l^k (\xi_1, \xi_2) = (-1)^k \deg (\xi_1 \xi_2 l^{d-2k})$.

		\item The \textbf{primitive subspace} of $A_f^k$ associated to $l$ is the subspace
		\[
			P_l^k := \{\xi \in A_f^k : l^{d-2k+1} \cdot \xi = 0\} \subseteq A_f^k.
		\]
	\end{enumerate}
\end{defn}

\begin{defn}
	Let $f$ be a homogeneous polynomial of degree $d$, let $k \leq d/2$ be a non-negative integer, and let $l \in A_f^1$ be a linear differential form. We define the following notions:
	\begin{enumerate}[label = (\alph*)]
		\item (Hard Lefschetz Property) We say $A_f$ satisfies $\HL_k$ with respect to $l$ if the Lefschetz operator $L_l^k$ is an isomorphism.

		\item (Hodge-Riemann Relations) We say $A_f$ satisfies $\HRR_k$ with respect to $l$ if the Hodge-Riemann form $Q_l^k$ is positive definite on the primitive subspace $P_l^k$. 
	\end{enumerate}
\end{defn}

Sometimes, instead of saying $A_f$ satisfies Hodge-Riemann Relations or the Hard Lefschetz Property, we will say that $f$ satisfies Hodge-Riemann Relations or the Hard Lefschetz property. For any $a \in \RR^n$, we can define the linear differential form $l_a := a_1 \partial_1 + \ldots + a_n \partial_n$. We say that $f$ satisfies $\HL$ or $\HRR$ with respect to $a$ if and only if it satisfies $\HL$ or $\HRR$ with respect to $l_a$. Most applications of the Hodge-Riemann Relations only end up using the relations up to degree $k \leq 1$. (GIVE EXAMPLE OF AN APPLICATION WHICH USES HIGHER DIMENSION,, maybe Chris Eur???) 

\begin{prop} [Lemma 3.4 in (CITE MURAI)] \label{conditions-for-HL-HRR}
	Let $f \in \RR[x_1, \ldots, x_n]$ be a homogeneous polynomial of degree $d \geq 2$ and $a \in \RR^n$. Assume that $f(a) > 0$. Then, 
	\begin{enumerate}[label = (\alph*)]
		\item $A_f$ has $\HL_1$ with respect to $l_a$ if and only $Q_{l_a}^1$ is non-degenerate. 

		\item $A_f$ has $\HRR_1$ with respect to $l_a$ if and only if $-Q_{l_a}^1$ has signature $(+, -, \ldots, -)$. 
	\end{enumerate}
\end{prop}

\begin{proof}
	We include a proof for completeness. We first prove the statement in (a). Suppose that $A_f$ has $\HL_1$ with respect to $l_a$. We have the following commutative diagram:
	\[\begin{tikzcd}
	{A_f^1 \times A_f^1} && {A_f^1 \times A_f^{d-1}} \\
	& {\mathbb{R}}
	\arrow["{\text{id} \times L_{l_a}^1}", from=1-1, to=1-3]
	\arrow["{-Q_{l_a}^1}"', from=1-1, to=2-2]
	\arrow[from=1-3, to=2-2]
\end{tikzcd}\]
where the missing mapping is multiplication. If $A_f$ has $\HL_1$ with respect to $l_a$, then the top map between $A_f^1 \times A_f^1 \to A_f^1 \times A_f^{d-1}$ is an bijection. Thus the non-degeneracy of $Q_{l_a}^1$ follows from the non-degeneracy of the multiplication pairing as stated in Proposition~\ref{chow-ring-is-a-PD-algebra}. Now, suppose that $Q_{l_a}^1$ is non-degenerate. Then, the map $B : A_f^1 \to (A_f^1)^*$ defined by $\xi \mapsto -Q_{l_a}^1(\xi, \cdot)$ is given by $m(L_{l_a}^1 \xi, \cdot)$ where $m : A_f^1 \to A_f^{d-1} \to \RR$ is the multiplication map. This is the composition of $A_f^1 \to A_f^{d-1} \to (A_f^1)_*$ where the first map is $L_{l_a}^1$ and the second map is injective from the non-degeneracy of the multiplication map. This proves that $L_{l_a}^1$ is injective. From Corollary~\ref{same-dimensions}, the map $L_{l_a}^1$ is an isomorphism. This suffices for the proof of (a). \\

To prove (b), consider the commutative diagram
\[\begin{tikzcd}
	{\mathbb{R}} & {A^0_f} & {A_f^1} & {A_f^{d-1}} & {A_f^d} & {\mathbb{R}}
	\arrow["{\times l_a}", from=1-2, to=1-3]
	\arrow["{L_{l_a}^1}", from=1-3, to=1-4]
	\arrow["{\times l_a}", from=1-4, to=1-5]
	\arrow["\simeq", from=1-1, to=1-2]
	\arrow["\simeq", from=1-5, to=1-6]
	\arrow["L_{l_a}^0"{description}, bend right= 18, from=1-2, to=1-5]
\end{tikzcd}\]
Note that $L_{l_a}^0$ is an isomorphism because 
\[
	\deg L_{l_a}^0 (1) = l_a^d (f)  = d! f(a) \neq 0.
\]
Thus, we have $A_f^1 = \RR l_a \oplus P_l^1$ where the direct sum is orthogonal over the Hodge-Riemann form by definition of the primitive subspace. Now, note that 
\[
	-Q_{l_a}^1(l_a, l_a) = l_a^d (f) = -d! f(a) > 0. 
\]
Thus, the signature of $-Q_{l_a}^1$ is $(+, -, \ldots, -)$ if and only if $Q_{l_a}^1$ is positive definite over the primitive subspace if and only if $A_f$ satisfies $\HRR_1$ with respect to $l_a$. 
\end{proof}

From Sylvester's Law of Inertia and the fact that is Hessians of Lorentzian polynomials always have at most one positive eigenvalue on the positive orthant. (cite in previous part of thesis maybe)
\begin{lem}[Lemma 3.5 in \cite{MNY}] \label{lorentzian-HL-iff-HRR}
	If $f \in \RR [x_1, \ldots, x_n]$ is Lorentzian, then for any $a \in \RR_{\geq 0}^n$ with $f(a) > 0$, $A_f^1$ has $\HL_1$ with respect to $l_a$ if and only if $f$ has the $\HRR_1$ with respect to $l_a$. 
\end{lem}
\section{Local Hodge-Riemann Relations}

Suppose that we are trying to prove the Hodge-Riemann Relations property for a Lorentzian polynomial. To do so, it is actually enough to show that it satisfies a ``local'' version of the Hodge-Riemann relations. This will imply that it satisfies the Hard Lefschetz Property, which by Lemma~\ref{lorentzian-HL-iff-HRR} is enough to prove that it satisfies Hodge-Riemann relations. We define the local $\HRR$ as in \cite{MNY}. 

\begin{defn}
	A homogeneous polynomial $f \in \RR[x_1, \ldots, x_n]$ of degree $d \geq 2k+1$ satisfies the \textbf{local} $\HRR_k$ with respect to a form $l \in A_f^1$ if for all $i \in [n]$, either $\partial_i f = 0$ or $\partial_i f$ satisfies $\HRR_k$ with respect to $l$. 
\end{defn}

One existing inductive mechanism for proving Hodge-Riemann relations is Lemma~\ref{local-hrr-mechanism}. 

\begin{lem} [Lemma 3.7 in \cite{MNY}] \label{local-hrr-mechanism}
	Let $f \in \RR_{\geq 0}[x_1, \ldots, x_n]$ be a homogeneous polynomial of degree $d$ and $k$ a positive integer with $d \geq 2k+1$, and $a = (a_1, \ldots, a_n) \in \RR^n$. Suppose that $f$ has the local $\HRR_k$ with respect to $l_a$. 
	\begin{enumerate}[label = (\roman*)]
		\item If $a \in \RR_{> 0}^n$, then $A_f$ has the $\HL_k$ with respect to $l_a$. 
		\item If $a_1 = 0, a_2, \ldots, a_n > 0$ and $\{\xi \in A_f^k : \partial_i \xi = 0 \text{ for $i = 2, \ldots, n$}\} = \{0\}$, then $A_f$ has the $\HL_k$ with respect to $l_a$. 
	\end{enumerate}
\end{lem}

This is effective in proving that all Lorentzian polynomials satisfy $\HRR_1$ with respect to $l_a$ for any $a \in \RR^n_{> 0}$. Indeed, this is easy to prove for Lorentzian polynomials of small degree. For an arbitrary Lorentzian polynomial, all of its partial derivatives are also Lorentzian (CITE THIS RESULT SOMEWHERE IN THE THESIS). Thus, by the inductive hypothesis all of these polynomials satsisfy $\HRR_1$. But this implies from Lemma~\ref{local-hrr-mechanism} that the original polynomial satisfies $\HL_1$, hence $\HRR_1$ since it is Lorentzian. This proves Theorem~\ref{lorentzian-satisfies-HRR}.

\begin{thm} [Theorem 3.8 in \cite{MNY}] \label{lorentzian-satisfies-HRR}
	If $f \in \RR[x_1, \ldots, x_n]$ is Lorentzian, then $f$ has $\HRR_1$ with respect to $l_a$ for any $a \in \RR_{> 0}^n$. 
\end{thm}

If the value of $a$ is not restricted to the positive orthant and additionally we know that $\partial_1, \ldots, \partial_n$, then we have the following computational result. 

\begin{cor} \label{partial-independent-implies-hessian}
	Let $f$ be a homogeneous polynomial of degree $d \geq 2$. If $\partial_1 f, \ldots, \partial_n f$ are linearly independent in $\RR[x_1, \ldots, x_n]$ and $f(a) > 0$ for some $a \in \RR^n$, then $A_f$ satisfies $\HRR_1$ with respect to $l_a$ if and only if $\Hess_f|_{x = a}$ has signature $(+, -, \ldots, -)$. 
\end{cor}

\begin{proof}
	Since $\partial_1 f_1, \ldots, \partial_n f_n$ are linearly independent, the partials $\partial_i$ form a basis for $A_f^1$. Thus, the signature of $Q_{l_a}^1$ is actually the signature of matrix of $Q_{l_a}^1$ with respect to the set $\{\partial_1, \ldots, \partial_n\}.$ We have 
	\[
		-Q_{l_a}^1(\partial_i, \partial_j) = \partial_i \partial_j l_a^{d-2} f =  l_a^{d-2} \partial_i \partial_j f = (d-2)! \partial_i \partial_j f(a).
	\]
	This proves that the signature of $-Q^1_{l_a}$ is the same as the signature of $\Hess_f |_{x = a}$.  We are done from Lemma~\ref{conditions-for-HL-HRR}(b).
\end{proof}
\section{Chow Ring of the Basis Generating Polynomial}

(maybe rename to Gorenstein ring? you should ask June)
\begin{defn}
	Let $M$ be a matroid. We define $A^\bullet(M) := A^\bullet_{f_M}$ to be the Chow ring of the basis generating polynomial of $M$. We then define $S_M := \RR [ x_e : e \in E]$ and $\Ann_M := \Ann_{S_M}(f_M)$. 
\end{defn}

As we have defined the Chow Ring, it becomes an excellent tool to study the basis generating polynomial of a matroid. This is because the Chow Ring of a matroid only depends on its simplification. Indeed, if $a, b \in E(M)$ are parallel elements, then $\partial_a - \partial_b$ gets annihilated and if $e \in E(M)$ is a loop then $\partial_e$ gets annihilated. We will describe this isomorphism in Theorem~\ref{only-simplification-matters}. Before proving this result, we first describe some common elements in the ideal $\Ann_M$. 

\begin{prop}[Proposition 3.1 in \cite{MN-gorenstein}]
	The annihilator $\Ann_M$ contains the elements
	\[
		\Lambda_M := \{x_e^2 : e \in E(M)\} \cup \{x^S : S \notin \mcI (M)\} \cup \{x^A - x^{A'} : A, A' \in \mcI(M), \overline{A} = \overline{A'}\}.
	\]
\end{prop}



\begin{example}
	The elements $\Lambda_M$ unfortunately do not generate $\Ann_M$ in general. In \cite{MN-gorenstein}, they give an example of matroid which is the matroid described by the linear independence of the following five vectors. 
	\begin{align*}
	 	e_1 & = (1, 0, 0) \\
	 	e_2 & = (0, 1, 0) \\
	 	e_3 & = (0, 0, 1) \\
	 	e_4 & = (1, 1, 0) \\
	 	e_5 & = (0, 1, 1).
	\end{align*}
	The bases are $123, 125, 134, 135$, and $145$. From Example 3.5 in \cite{MN-gorenstein}, it happens that 
	\[
		\Ann_M = \langle \Lambda_M \rangle + (\partial_1 \partial_3 + \partial_{4} \partial_5 - \partial_1 \partial_5 - \partial_3 \partial_4).
	\]
\end{example}


Let $M = (E, \mathcal{I})$ be a matroid and let $\widetilde{M}$ be its simplification. Recall that $\widetilde{M}$ is a matroid on the ground set of rank-$1$ flats $E(\widetilde{M}) = \{\bar{x} : x \in E(M) \backslash E_0 (M)\}$ whose independent sets consist of the subsets of $E(\widetilde{M})$ where by taking one representative from each rank-$1$ flat, we get an independent set of $M$. We can define $\phi : S_M \to S_{\widetilde{M}}$ and $\psi : S_{\widetilde{M}} \to S_M$ by 
\begin{align*}
	\phi (\partial_{x_e}) := \partial_{\overline{x_e}}, \quad \text{and} \quad \psi (\partial_{\overline{x}}) := \frac{1}{|\overline{x}|} \sum_{e \in \overline{x}} \partial_{x_e}.
\end{align*}
The entire maps $\phi$ and $\psi$ are then defined by using the universal property of polynomial rings to extend over the whole space. 

\begin{thm} \label{only-simplification-matters}
	The maps $\phi : S_M \to S_{\widetilde{M}}$ and $\psi : S_{\widetilde{M}} \to S_M$ induce isomorphisms between $A(M)$ and $A(\widetilde{M})$. 
\end{thm}

\begin{proof}
	We first prove that $\phi$ and $\psi$ induce homomorphisms between the Chow rings. To show that $\phi$ induces a homomorphism, consider the diagram in Equation~\ref{extension-1}.
	\begin{equation} \label{extension-1}
			\begin{tikzcd}
				{S_M} && {S_{\widetilde{M}}} && {A(\widetilde{M})} \\
				\\
				&& {A(M)}
				\arrow["\phi", from=1-1, to=1-3]
				\arrow["{\pi_{\widetilde{M}}}", from=1-3, to=1-5]
				\arrow["{\pi_M}"', from=1-1, to=3-3]
				\arrow["{\exists ! \Phi}"', dotted, from=3-3, to=1-5]
			\end{tikzcd}
	\end{equation}
		

Let $\xi \in S_M$ be an element satisfying $\xi (f_M) = 0$. We will prove that $\phi(\xi) (f_{\widetilde{M}}) = 0$. In other words, we want to prove that $\Ann_M \subseteq \ker \pi_{\widetilde{M}} \circ \phi$. From Proposition~\ref{homogeneous-parts}, it suffices to consider the case where $\xi$ is homogeneous. Let $e_1, \ldots, e_s$ be representatives of all parallel classes. Then, we have that $M = \overline{e_1} \sqcup \ldots \sqcup \overline{e_s} \cup E_0$ where $E_0$ denotes the loops in $M$. In terms of the basis generating polynomials, we have 
\begin{align*}
	f_{\widetilde{M}}(x_{\overline{e_1}}, \ldots, x_{\overline{e_s}}) & = \sum_{\substack{1 \leq i_1 < \ldots < i_r \leq s \\ \{\overline{e_{i_1}}, \ldots, \overline{e_{i_r}}\} \in \mathcal{B}(\widetilde{M})}} x_{\overline{e_{i_1}}} \ldots x_{\overline{e_{i_r}}} \\ 
	f_M(x_1, \ldots, x_n) & = f_{\widetilde{M}} \left ( y_1, \ldots, y_s \right )
\end{align*}
where for $1 \leq i \leq s$, we define $y_i := \sum_{e \in \overline{e_i}} x_e$. Since $\xi$ is homogeneous of degree $k$, we can write it in the form $\xi = \sum_{\substack{\alpha \subseteq [n] \\ |\alpha| = k}} c_\alpha \partial^\alpha$. Then, we have 
\begin{align} \label{karen}
	\xi (f_M) & = \sum_{\beta \in \mathcal{B}} \xi (x^\beta) = \sum_{\beta \in \mathcal{B}(M)} \sum_{\substack{\alpha \subseteq [n] \\ |\alpha| = k}} c_\alpha \partial^\alpha x^\beta = \sum_{\gamma \in \mathcal{I}_{r-k}(M)} \left ( \sum_{ \substack{\alpha \in \mathcal{I}_k \\ \alpha \cup \gamma \in \mathcal{I}_r(M)}} c_\alpha  \right ) x^\gamma. 
\end{align}

Since $\xi(f_M) = 0$, we know that all of the coefficients on the right hand side of Equation~\ref{karen} are equal to $0$. Thus, for any $\gamma \in \mathcal{I}_{r-k}(M)$, we have 
\[
	\sum_{\substack{\alpha \in I_k \\ \alpha \cup \gamma \in \mathcal{I}_r(M)}} c_\alpha = 0.
\]
On the other hand, we have 
\[
	\phi (\xi) = \sum_{\substack{\alpha \subseteq [n] \\ |\alpha| = k}} c_\alpha \prod_{e \in \alpha} \partial_{\overline{e}} = \sum_{\beta \in \mathcal{I}_{k}(\widetilde{M})} \left ( \sum_{\alpha \in \text{fiber}(\beta)} c_\alpha \right ) \partial^\beta. 
\]
We can let this differential act on $f_{\widetilde{M}}$ to get the expression
\begin{align} \label{karen-math}
	\phi(\xi) (f_{\widetilde{M}}) & = \sum_{\gamma \in \mathcal{I}_{r-k}(\widetilde{M})} \left ( \sum_{\substack{\beta \in \mcI_k(\widetilde{M}) \\ \beta \cup \gamma \in \mathcal{B}(\widetilde{M})}} \sum_{\alpha \in \text{fiber}(\beta)} c_\alpha \right ) x^\gamma = \sum_{\gamma \in \mathcal{I}_{r-k}(\widetilde{M})} \left ( \sum_{\substack{\alpha \in \mcI_k(M) \\ \alpha \cup \gamma_0 \in \mathcal{B}(M)}} c_\alpha \right ) x^\gamma = 0.
\end{align}
In Equation~\ref{karen-math}, the independent set $\gamma_0 \in \text{fiber}(\gamma)$ is an arbitrary element in the fiber of $\gamma$. This proves that $\Ann_M \subseteq \ker \pi_{\widetilde{M}} \circ \phi$. Thus, there is a unique ring homomorphism $\Phi : A(M) \to A(\widetilde{M})$ which makes Equation~\ref{extension-1} commute. \\

To prove that $\psi$ induces a map between the Chow rings, consider the diagram in Equation~\ref{extension-2}. 
\begin{equation} \label{extension-2}
\begin{tikzcd}
	{S_{\widetilde{M}}} && {S_M} && {A(M)} \\
	\\
	&& {A(\widetilde{M})}
	\arrow["{\pi_{\widetilde{M}}}"', from=1-1, to=3-3]
	\arrow["\psi", from=1-1, to=1-3]
	\arrow["{\pi_M}"', from=1-3, to=1-5]
	\arrow["{\exists! \Psi}"', dotted, from=3-3, to=1-5]
\end{tikzcd}
\end{equation}

Consider a differential $\xi \in S_{\widetilde{M}}$ satisfying $\xi(f_{\widetilde{M}}) = 0$. We can write this as $\xi = \sum_{\alpha \in \mathcal{I}_k (\widetilde{M})} c_\alpha \partial^\alpha$ for some real constants $c_\alpha$. Then, its image under $\psi$ is equal to
\[
	\psi(\xi) = \sum_{\alpha \in \mathcal{I}_k (\widetilde{M})} \frac{c_\alpha}{\prod_{e \in \alpha} |e|}\sum_{\beta \in \text{fiber}(\alpha)} \partial^\beta.
\]
Fix a $\alpha \in \mathcal{I}_k(\widetilde{M})$ and a $\beta \in \text{fiber}(\alpha)$. Since $\partial y_i / \partial x_e = \1_{e \in \overline{e_i}}$, we have  
\begin{align*}
	\partial^\beta f_M (x_1, \ldots, x_n) & = \partial^\beta f_{\widetilde{M}} (y_1, \ldots, y_s) = \partial^\alpha f_{\widetilde{M}} (x_1, \ldots, x_s) |_{x_1 = y_1, \ldots, x_s = y_s} = 0.
\end{align*}
Thus, we have that $\psi (\xi)(f_M) = 0$ and $\Ann_{\widetilde{M}} \subseteq \ker \pi_M \circ \psi$. This proves that there is a unique ring homomrphism $\Psi : A(\widetilde{M}) \to A(M)$ which causes the diagram in Equation~\ref{extension-2} to commute. Since the maps $\Psi$ and $\Phi$ are inverses of each other, they are both isomorphisms. This suffices for the proof.
\end{proof}

From Theorem~\ref{only-simplification-matters}, we get Corollary~\ref{complex-isomorphism} and Corollary~\ref{simplification-of-HRR} immediately. 

\begin{cor} \label{complex-isomorphism}
	Let $M = (E, \mathcal{I})$ be a matroid. For any $a \in \RR^E$, we can define the linear form $l_a := \sum_{e \in E} a_e \cdot \partial_{x_e} \in A^1(M)$. Let $\widetilde{l_a} := \Phi(l_a) = \sum_{e \in E} a_e \cdot \partial_{x_{\overline{e}}} \in A^1(\widetilde{M})$. Then, the following diagram is an isomorphism of chain complexes:
	\[\begin{tikzcd}
	\ldots & {A^{i-1}(M)} & {A^i(M)} & {A^{i+1}(M)} & \ldots \\
	\ldots & {A^{i-1}(\widetilde{M})} & {A^i(\widetilde{M})} & {A^{i+1}(\widetilde{M})} & \ldots
	\arrow["{\times l_a}", from=1-1, to=1-2]
	\arrow["{\times \widetilde{l_a}}", from=2-1, to=2-2]
	\arrow["\Phi"', from=1-2, to=2-2]
	\arrow["{\times l_a}", from=1-2, to=1-3]
	\arrow["{\times \widetilde{l_a}}", from=2-2, to=2-3]
	\arrow["{\times l_a}", from=1-3, to=1-4]
	\arrow["{\times \widetilde{l_a}}", from=2-3, to=2-4]
	\arrow["\Phi"', from=1-3, to=2-3]
	\arrow["\Phi"', from=1-4, to=2-4]
	\arrow["{\times l_a}", from=1-4, to=1-5]
	\arrow["{\times \widetilde{l_a}}", from=2-4, to=2-5]
\end{tikzcd}\]
 \end{cor}

\begin{cor} \label{simplification-of-HRR}
	Let $M = (E, \mathcal{I})$ be a matroid. Then $A(M)$ satisfies $\HRR_k$ with respect to $l$ if and only if $A(\widetilde{M})$ satisfies $\HRR_k$ with respect to $\Phi(l)$. 
\end{cor}

From Theorem~\ref{only-simplification-matters}, Corollary~\ref{complex-isomorphism}, and Corollary~\ref{simplification-of-HRR}, we are led to study $A(M)$ in the case where $\widetilde{M}$ is simple. In \cite{MNY}, Satoshi Murai, Takahiro Nagaoka, and Akiko Yazawa prove that if $M$ is a simple matroid on $[n]$, then $\dim A^1(M) = n$ with basis $\partial_1, \ldots, \partial_n$.

\begin{lem} [Theorem 2.5 in \cite{MNY}] \label{simple-partial-independent}
	If $M = ([n], \mcI)$ is simple, then $\partial_1, \ldots, \partial_n$ is a basis of $A^1(M)$.
\end{lem}

From Corollary~\ref{partial-independent-implies-hessian}, this implies that when $M$ is simple, the Hessian of the basis generating polynomial $f_M$ has signature $(+,-, \ldots, -)$ at every point in the positive orthant. Thus, the basis generating polynomial for a simple matroid is \textbf{strictly} log-concave on the positive orthant. We are interested in log-concavity on the facets of the positive orthant. In other words, we want to extend the region in which we know $A(M)$ satisfies $\HRR_1$. From Lemma~\ref{lorentzian-satisfies-HRR}, we already know that $A(M)$ satisfies $\HRR_1$ on $\RR_{> 0}^n$. For a matroid $M = (E, \mathcal{I})$, we can write
\[	
	\bd \RR^E = \bigcup_{e \in E} H_e
\]
where $H_e := \{x \in \RR_{\geq 0}^E : x_e = 0 \}$. In the next section, we prove necessarily and sufficient conditions for the basis generating polynomial to satisfy $\HRR_1$ on the relative interior of one of the facets. \\

Before moving on to the study of $\HRR_1$ on the facets of the positive orthant, our current results allow us to find necessary and sufficiently conditions to tell us when $A(M)$ satisfies $\HRR_1$ with respect to $a \in \RR^n$ in the case $f_M(a) > 0$ and $M$ is simple.

\begin{cor} \label{simple-matroids-HRR-condition}
	Let $M = (E, \mcI)$ be a simple matroid of rank $r \geq 2$. If $a \in \RR_{\geq 0}^E$ satisfies $f_M(a) > 0$, then $A(M)$ satisfies $\HRR_1$ with respect to $l_a$ if and only if $\Hess_{f_M} |_{x = a}$ is non-singular.
\end{cor} 

\begin{proof}
	This follows immediately from Lemma~\ref{lorentzian-HL-iff-HRR}, Corollary~\ref{partial-independent-implies-hessian}, and Lemma~\ref{simple-partial-independent}.
\end{proof}

\begin{thm}
	Let $M = (E, \mcI)$ be a simple matroid of rank $r \geq 2$. If $a \in \RR_{\geq 0}^E$ satisfies $f_M(a) > 0$ and $a_e = 0$ for some $e \in E$ which is not a co-loop, then $A(M)$ satisfies $\HRR_1$ with respect to $l_a$ if and only if 
	\[
		\det \left ( \nabla f_{M / e}^{\mathsf{T}} \cdot \Hess^{-1}_{f_{M \backslash e}} \cdot \nabla f_{M / e} \right ) |_{x = a} \neq 0.
	\]
\end{thm}

\begin{proof}
	Without loss of generality, we can assume that $E(M) = [n]$ and $e = n$. In particular, this means that $a = (a_1, \ldots, a_{n-1}, 0) \in \RR^n$ with $a_i \geq 0$ for all $i$. From Corollary~\ref{simple-matroids-HRR-condition}, $\HRR_1$ is satisfied if and only if the Hessian is non-singular. To compute the Hessian at $x = a$, note that because $n$ is a co-loop, we can rewrite the basis generating polynomial as 
	\[
		f_M = x_n f_{M / n} + f_{M \backslash n}.
	\]
	The Hessian when computed at $(a_1, \ldots, a_{n-1}, 0)$ will be equal to 
	\[
		\Hess_{f_M} = \begin{bmatrix}
			\Hess_{f_{M \backslash n}} & \nabla f_{M/n} \\
			(\nabla f_{M/n})^{\mathsf{T}} & 0
		\end{bmatrix}.
	\]
	We know that $M \backslash n$ is still simple. Thus $\Hess_{f_{M \backslash n}}$ is invertible since it has the same signature as the Hodge-Riemann form. Recall the following linear algebraic lemma. 
	\begin{lem}
		When $D$ is invertible and $A$ is a square matrix, we have
		\[
			\det \begin{bmatrix} A & B \\ C & D \end{bmatrix} = \det (A - B D^{-1} C) \det (D).
		\]
	\end{lem}
	From the lemma, we have 
	\[
		\det \Hess_{f_M} = \det \left ( \nabla f_{M / n}^{\mathsf{T}} \cdot \Hess^{-1}_{f_{M \backslash n}} \cdot \nabla f_{M / n} \right ).
	\]
	This suffices for the proof. 
\end{proof}
\section{Hodge-Riemann relations on the facets of the positive orthant}

\begin{lem} \label{contracting-stays-not-a-coloop}
	Let $M = (E, \mathcal{I})$ be a matroid satisfying $\rank (M) \geq 2$. If $e \in M$ is not a co-loop of $M$, then $e$ will not be a co-loop of $M / i$ for any $i \in E \backslash e$. 
\end{lem}

\begin{proof}
	We assume that $e$ is not a loop since otherwise it would not be a co-loop of any non-trivial matroid. We can also assume that $i$ is not a loop otherwise the statement is trivial. Suppose for the sake of contradiction that $e$ is a co-loop of $M/i$ and $i$ is not a loop. In the language of the original matroid, this means that any basis containing $i$ also contains $e$. Since $e$ is not a co-loop, the deletion $M / e$ will have $\rank (M / e) = \rank M$. Since $i$ is not a loop, it is contained in a basis of $M / e$. This would be a basis in the original matroid $M$ not containing $e$. This is a contradiction. 
\end{proof}

(what was the socle interpretation?)
\begin{thm} \label{socle-socle-socle}
	Let $M = (E, \mcI)$ be a matroid satisfying $\rank (M) \geq 3$. Let $S \subseteq E(M)$ be a subset with $\rank (S) \leq \rank (M)-2$. Then 
	\[
		\{\xi \in A_f^1 : \xi (\partial_i f) = 0 \text{ for } i \in E \backslash S\} = \{0\}.
	\]
\end{thm}

\begin{proof}
	Let $r = \rank (M) \geq 3$. We want to prove that if a linear form $\xi = \sum_{e \in E} c_e \cdot \partial_e$ satisfies $\xi (\partial_e f_M) = 0$ for all $e \in E \backslash S$, then we have $\xi (f_M) = 0$. For $i \in E \backslash S$, we have 
	\begin{equation} \label{socle-calculation-1}
		0 = \xi (\partial_i f_M) = \sum_{e \in E} c_e \partial_e \partial_i f_M = \sum_{e \in E} c_e \sum_{\substack{\alpha \in \mcI_{r-2}(M) \\ \alpha \cup \{e,i\} \in \mcI_r (M)}} x^\alpha = \sum_{\alpha \in \mcI_{r-2}(M)} \left ( \sum_{\substack{e \in E \\ \alpha \cup \{i, e\} \in \mcI_r(M)}} c_e \right ) x^\alpha. 
	\end{equation}
	By setting all of the coefficients of the right hand side of Equation~\ref{socle-calculation-1}, we have that 
	\[
		\sum_{\substack{e \in E \\ \alpha \cup \{i, e\} \in \mcI_r (M)}} c_e = 0
	\]
	for all $\alpha \in \mcI_{r-2}(M)$ and $i \in E \backslash S$. For any $\beta \in \mcI_{r-1}(M)$, we know that $\beta \not \subseteq S$ since $\rank (\beta) = r-1 > \rank (S)$. There exists some $i \in \beta \backslash S$. Thus, we can write $\beta = \alpha \cup \{i\}$ where $\alpha \in \mcI_{r-2}(M)$ and $i \in E \backslash S$. This proves that for any $\beta \in \mcI_{r-1}(M)$, we have
	\[
		\sum_{\substack{e \in E \\ \beta \cup \{e\} \in \mcI_{r}}} c_e = \sum_{\substack{e \in E \\ \alpha \cup \{i, e\} \in \mcI_{r}}} c_e = 0.
	\]
	Finally, we have that 
	\[
		\xi(f_M) = \sum_{e \in E} c_e \partial_e f_M = \sum_{e \in E} c_e \sum_{\substack{\beta \in \mcI_{r-1} \\ \beta \cup \{e\} \in \mcI_r}} x^\beta = \sum_{\beta \in \mcI_{r-1}} \left ( \sum_{\substack{e \in E \\ \beta \cup \{e\} \in \mcI_r}} c_e \right ) x^\beta = 0.
	\]
	This suffices for the proof of the Theorem. 
\end{proof}

\begin{thm}[Higher Degree Socles]
	Let $M = (E, \mcI)$ be a matroid. Let $S \subseteq E$ be a subset such that $\rank(S) \leq \rank(M) - k - 1$. Then, 
	\[
		\{\xi \in A^k(M) : \xi (\partial_e f_M) = 0 \text{ for all } e \in E \backslash S\} = \{0\}.
	\]
\end{thm}

\begin{proof}
	Any $\xi \in A^k(M)$ can be written as $\xi = \sum_{\alpha \in \mcI_k} c_\alpha \partial^\alpha$. For any $e \in E \backslash S$, we have 
	\begin{align*}
		0 = \xi \partial_i f_M = \sum_{\alpha \in \mcI_k} c_\alpha \partial_e \partial^\alpha f_M = \sum_{\alpha \in \mcI_k} c_\alpha \sum_{\substack{\gamma \in \mcI_{r-k-1} \\ \gamma \cup \alpha \cup \{e\} \in \mcI_r}} x^\gamma = \sum_{\gamma \in \mcI_{r-k-1}} \left ( \sum_{\substack{\alpha \in \mcI_k \\ \gamma \cup \alpha \cup \{e\} \in \mcI_r}} c_\alpha \right ) x^\gamma.
	\end{align*}
	This implies that for all $\gamma \in \mcI_{r-k-1}$ and $e \in E \backslash S$, we have 
	\[
		\sum_{\substack{\alpha \in \mcI_k \\ \gamma \cup \alpha \cup \{e\} \in \mcI_r}} c_\alpha = 0
	\]
	for all $\gamma \in \mcI_{r-k-1}$ and $e \in E \backslash S$. Let $\beta \in \mcI_{r-k}$ be an arbitrary independent set. Note that $\rank (\beta) = r-k > \rank (S)$. Thus, we cannot have $\beta \subseteq S$. This implies that we can find $e \in \beta \backslash S$ such that $\beta = \alpha \cup \{e\}$ for $e \in E \backslash S$. Thus, 
	\[
		\sum_{\substack{\alpha \in \mcI_k \\ \beta \cup \alpha \in \mcI_r}} c_\alpha = 0
	\]
	for all $\beta \in \mcI_{r-k}$. Then, we have 
	\[
		\xi (f_M) = \sum_{\alpha \in \mcI_k} c_\alpha \partial^\alpha f_M = \sum_{\alpha \in \mcI_k} c_\alpha \sum_{\substack{\beta \in \mcI_{r-k} \\ \beta \cup \alpha \in \mcI_r}} x^\beta = \sum_{\beta \in \mcI_{r-k}} \left ( \sum_{\substack{\alpha \in \mcI_k \\ \beta \cup \alpha \in \mcI_r}} c_\alpha \right ) x^\beta = 0.
	\]
	This suffices for the proof. 
\end{proof}

\begin{thm} \label{main-theorem}
	Let $M = (E, \mcI)$ be a matroid which satisfies $\rank (M) \geq 2$. For any $e \in E(M)$, the basis generating polynomial $f_M$ satisfies $\HRR_1$ on $\relint (H_e)$ if and only if $e$ is not a co-loop.
\end{thm}

\begin{proof}
	Without loss of generality, let $M$ be a matroid on the set $[n]$ and let $e = 1$. Then, we want to prove that whenever $a_2, \ldots, a_n > 0$, the ring $A(M)$ satisfies $\HRR_1$ on $a = (0, a_2, \ldots, a_n)$ if and only if $e$ is not a co-loop. We first prove that if $1$ is not a co-loop, then $A(M)$ satisfies $\HRR_1$. To prove this, we induct on the rank of $M$. For the base case $\rank (M) = 2$, Corollary~\ref{simplification-of-HRR} implies that it suffices to prove that $A(\widetilde{M})$ satisfies $\HRR_1$ on $\Phi(l_a)$. The only simple matroid of rank $2$ is the uniform matroid of rank $2$. From Corollary~\ref{simple-matroids-HRR-condition}, it suffices to check that the signature of the Hessian is $(+, -, \ldots, -)$. But the Hessian of a rank $2$ simple matroid at every point is the same matrix. If the matroid is on $n$ elements, the Hessian matrix is 
	\[
		A(K_n) = \begin{bmatrix} 
			0 & 1 & 1 & \ldots & 1 \\
			1 & 0 & 1 & \ldots & 1 \\
			1 & 1 & 0 & \ldots & 1 \\
			\vdots & \vdots & \vdots & \ddots & \vdots \\
			1 & 1 & 1 & \ldots & 0
		\end{bmatrix}.
	\]
	This happens to be the adjancency matrix of a complete graph. From Proposition 1.5 in \cite{Stanley-alg-combo}, this matrix has an eigenvalue of $-1$ with multiplicity $n-1$ and an eigenvalue of $n-1$ with multiplicity $1$. Hence, its signature is $(+, -, \ldots, -)$ which proves the base case. Now suppose that the claim is true for all matroids of rank less than $r$. Let $M$ be a matroid of rank $r$. We want to prove that the claim is true for $M$. By the same reasoning, it suffices to prove the result when $M$ is loopless. For all $i \in E \backslash \{1\}$, we know that $e$ is not a co-loop of $M / i$ from Lemma~\ref{contracting-stays-not-a-coloop}. Since $M$ is simple, we know that $\rank (M / i) = \rank (M) - 1 < \rank (M)$. Hence, from the inductive hypothesis, we know that $A(M/i) = A_{\partial_i f_M}^\bullet$ satisfies $\HRR_1$ for $l_a$. \\

	Now, we have enough information to directly prove that $A(M)$ satisfies $\HRR_1$ with respect to $l_a$. From Lemma~\ref{lorentzian-HL-iff-HRR}, it suffices to prove that $A(M)$ satisfies $\HL_1$ with respect to $l_a$. Since $\dim_\RR A^1(M) = \dim_\RR A^{r-1}(M)$ from being a Poincare Duality algebra, it suffices to prove that the Lefschetz operator $L_{l_a}^1 : A^1(M) \to A^{r-1}(M)$ is injective. Let $\Xi \in A^1(M)$ be the kernel of $L_{l_a}^1$. This is the same as saying $\Xi l_a^{r-2} = 0$ in $A(M)$. We want to prove that $\Xi = 0$ in $A^1(M)$. Since $\Xi l_a^{r-2} = 0$ in $A(M)$, we have 
	\[
		0 = -Q_{l_a}^1 (\Xi, \Xi) = \deg_M (\Xi^2 \cdot l_a^{r-2}) = \sum_{i = 2}^n a_i \deg_M (\Xi^2 \cdot l_a^{r-3} \cdot \partial_i).
	\]
	Note that 
	\[
		\deg_M (\Xi^2 \cdot l_a^{r-3} \cdot \partial_i) = (\Xi^2 \cdot l_a^{r-3}) (\partial_i f_M) = \deg_{M / i} (\Xi^2 \cdot l_a^{r-3}) = - Q_{M/i} (\Xi, \Xi).
	\]
	where $Q_{M/i}$ is the Hodge-Riemann form of degree $1$ with respect to $l$ associated with $A(M/i)$. Thus, 
	\[
		\sum_{i = 2}^n a_i Q_{M/i}(\Xi, \Xi) = 0.
	\]
	In $A(M/i)$, the linear form $\Xi$ is in the primitive subspace. Hence, since we know that $A(M/i)$ satisfies $\HRR_1$ with respect to $l$, we know that the Hodge-Riemann form $Q_{M/i}$ is negative-definite on $\RR \Xi$. Since $a_i > 0$ for $i = 2, \ldots, n$, this implies that $Q_{M/i}(\Xi, \Xi) = 0$ for all such $i$. Hence $\Xi = 0$ in $A(M/i)$ for all $i \in [2, n]$. In terms of polynomials, this means that $\Xi (\partial_i f_M) = 0$ for all $i \in [2, n]$. From Theorem~\ref{socle-socle-socle}, this implies that $\Xi (f_M) = 0$. Thus, $A(M)$ satisfies $\HRR_1$ with respect to $l$. \\

	For the other direction, we will prove that if $e$ is a co-loop of $M$, then $f_M$ does not satisfy $\HRR_1$ on $\relint (H_e)$. Without loss of generality, we can suppose $E(M) = [n]$ and $e = 1$. The linear differential can be written as $l_a$ where $a = (0, a_2, \ldots, a_n)$ for $a_2, \ldots, a_n \geq 0$. It suffices to consider the case when $M$ is simple because the coefficient of $\partial_1$ in the simplification of $l_a$ will remain $0$. This is because co-loops have no parallel elements. In the simple case, the bottom $(n-1) \times (n-1)$ sub-matrix of $\Hess_{f_M}$ will be entirely $0$. This means that the Hodge-Riemann form will singular and $f_M$ cannot satisfy $\HRR_1$ on $\relint (H_e)$. This suffices for the proof. 
\end{proof}

With a similar type of proof, you can prove the following result. 
\begin{thm}
	Let $M = (E, \mcI)$ be a matroid which satisfies $\rank (M) \geq 2$. For any $S \subseteq E$, if $\rank(S) \leq \rank (M) - 2$ and $S$ contains no co-loops, then $f_M$ satisfies $\HRR_1$ on $\relint (H_S)$. 
\end{thm}

\begin{conj}
	For any matroid $M$, $A (M)$ has $\HL_k$ for all $k$ with respect to some linear form $l$. 
\end{conj}

\begin{lem}
	Let $A(M)$ satisfies $\HRR_i$ for $1 \leq i \leq k$, then, we have 
	\[
		\mathsf{sig}(Q_l^k) = \sum_{i = 0}^k (-1)^i (\dim A^i(M) - \dim A^{i-1}(M)).
	\]
\end{lem}
\begin{lem}
	Let $k \geq 2$ and $A(M)$ satisfies $\HRR_{i}$ and $\HL_{i}$ with respect to $l = l_a$ where $a \in \RR_{> 0}^n$ for all $1 \leq i \leq k-1$. Then
	\begin{enumerate}[label = (\alph*)]
		\item $A(M)$ satisfies $\HL_k$ with respect to $l$ if and only if $Q_l^k$ is non-degenerate on $A^k(M)$. 

		\item $A(M)$ satisfies $\HRR_k$ with respect to $l$ if and only if 
		\[
			\mathsf{sig}(Q_l^k) = \sum_{i = 0}^k (-1)^i (\dim A^i(M) - \dim A^{i-1}(M))
		\]
		where $A^{-1}(M) = 0$. Specifically, we want $Q_l^k$ to have 
		\[
			\frac{1}{2} \left ( \dim A^k(M) + \sum_{i = 0}^k (-1)^i (\dim A^i(M) - \dim A^{i-1}(M)) \right ) \text{ positive eigenvalues}
		\]
		and
		\[
			\frac{1}{2} \left ( \dim A^k(M) - \sum_{i = 0}^k (-1)^i (\dim A^i(M) - \dim A^{i-1}(M)) \right ) \text{ negative eigenvalues}.
		\]
	\end{enumerate}
\end{lem}
\chapter{Appendix}

\chapter{References}
\bibliographystyle{plain}
\bibliography{ref}
\end{document}
