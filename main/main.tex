\documentclass[12pt]{article}
\usepackage{style}
\usepackage[margin=1in]{geometry}
\usepackage[super]{nth}
\newcommand{\SLP}{\mathsf{SLP}}
\newcommand{\HRR}{\mathsf{HRR}}
\title{Ultra Log-concavity of Basis Partition Sequence}
\author{Alan Yan \\ Senior Thesis Advisor: Professor June Huh}
\begin{document}

\maketitle

\begin{abstract}
	Let $M = (E, \mcI)$ be a matroid of rank $r$ and let $E = A \sqcup B$ be a fixed partition of the ground set. For $0 \leq k \leq r$, let $N_k$ be the number of bases whose intersection with $A$ has size $k$. We prove that the sequence $\{N_k\}$ is ultra-log-concave. 
\end{abstract}
\tableofcontents

\newpage 

\section{Background Material}

\section{Matroids}



\subsection{Graphic Matroids}

The theory about Laplacians of graphs was taken from \cite{chung-spectral-graph-theory}.
\begin{defn}
	Given a graph $G$ with $n$ vertices $v_1, \ldots, v_n$, its Laplacian matrix $L \in \mathsf{Mat}_{n, n} (\ZZ)$ is defined element-wise as 
	\[
		L_{i, j} := \begin{cases}
			\deg (v_i) & \text{if $i = j$} \\
			- E_{v_i, v_j} & \text{if $i \neq j$ and $v_i$ is adjacent to $v_j$}
		\end{cases}
	\]
	where $E_{v_i, v_j}$ is the number of edges between $v_i$ and $v_j$.
\end{defn}
	
\begin{defn}
	Let $G = (V, E)$ be a graph and let $V = \{v_1, \ldots, v_n\}$ be an arbitrary ordering of the vertices. For this ordering, we can define a $|V| \times |E|$ matrix $B_{G}$ called the incidence matrix where the entry indexed by $v \in V$ and $e = \{v_i, v_j\} \in E$ where $v_i > v_j$ is equal to 
	\[
		B_{ve} = \begin{cases}
			1, & \text{if $v = v_i$} \\
			-1, & \text{if $v = v_j$} \\
			0, & \text{otherwise.}.
		\end{cases}
	\]
\end{defn}

\begin{prop}
	Let $G$ be a graph and let $L$ be its Laplacian. Then, for any arbitrary ordering on the vertices, we have $L = BB^T$. 
\end{prop}

\begin{proof}
	For each $e = \{v_i, v_j\} \in E$ with $v_i > v_j$, we define $\pi_e(v_i) = 1$ and $\pi_e(v_j) = -1$. We have that
	\[
		(BB^T)_{v,w} = \sum_{e \in E : v, w \in e} \pi_e(v) \pi_e(w).
	\]
	When $v = w$, then each summand is equal to $1$ and we get exactly $\deg (v_i)$. If $v$ and $w$ are not adjacent then the sum is empty and the entry is equal to zero. Otherwise, each summand is $-1$ and there are $E_{v_i, v_j}$ elements in the sum. This suffices for the proof of the proposition.
\end{proof}

\begin{defn}
	(Graphic Matroid)
\end{defn}


\begin{lem}[Example 5.4 in \cite{bapat_raghavan_1997}]
	The matrix $C$ obtained by deleting a row of $B$, say the last row, is called the \textit{reduced incidence matrix}. A set of columns of $B$ or $C$ are linearly independent if and only if the graph formed by the corresponding edges does not contain a cycle. 
\end{lem}

\begin{proof}
	See \cite{bapat_raghavan_1997} Example 5.4.
\end{proof}

\begin{lem}[Lemma 5.3.4 in \cite{bapat_raghavan_1997} (actually cite oxley for more citations)]
	The incidence matrix $B$ is totally unimodular. 
\end{lem}

\begin{proof}
	
\end{proof}

(To be added: Matroids, Mixed Volumes, Mixed Discriminants, Lorentzian Polynomials, Hodge Riemann Relations)

\section{Introduction}

The question is motivated by the results in \cite{STANLEY}. In our setting, we have a matroid $M = (E, \mcB)$ of rank $r$ and size $n$ where $\mcB$ is the set of bases. Fix a partition of the ground set $E = A \cup B$. From this, we can define the following combinatorial sequence
\[
	N_k := N_k(A) := \# \{U \in \mcB : |U \cap A| = k\}.
\]
In particular, Stanley uses the Alexandrov-Fenchel inequality to prove the following theorem. 
\begin{thm} \label{stanley-theorem}
	If $M$ is regular, then $N_k / \binom{r}{k}$ is log-concave. 
\end{thm}

\begin{proof}
	Let $\{v_1, \ldots, v_n\}$ be a unimodular coordinatization of the matroid $M$ where the vectors are in $\RR^r$. Consider the zonotopes
	\begin{align*}
		Z_A := \sum_{x \in A} [0, v_x] \\
		Z_B := \sum_{x \in B} [0, v_x].
	\end{align*}
	Then 
	\begin{align*}
		\Vol_r (\lambda Z_A + \mu Z_B) & = \Vol_r \left ( \sum_{x \in A} [0, \lambda v_x] + \sum_{x \in B} [0, \mu v_x] \right ) \\
		& = \sum_{k = 0}^r N_k \lambda^k \mu^{r-k}.
	\end{align*}
	This implies that 
	\[
		N_k = \binom{r}{k} V(Z_A[k], Z_B[r-k]). 
	\]
	This proves that $N_k / \binom{r}{k}$ is log-concave. 
\end{proof}

Stanley's proof relates the sequence $N_k$ to a sequence of mixed volume. The ultra-log-concave follows from the Alexandrov-Fenchel inequality. The relationship relies on the unimodular coordinazation that exists for regular matroids. It is unclear whether or not the result holds true for general matroids. There are three possible conjectures each of increasing strength. There is another proof of the theorem using mixed discriminants \cite{bapat_raghavan_1997}. 

\begin{proof}
	Let $M = (E, \mathcal{I})$ be a matroid of rank $r$ and consider a partition 
	\[
		E = S_1 \sqcup S_2 \sqcup \ldots \sqcup S_t
	\]
	of the matroid into $t$ sets. For $(r_1, \ldots, r_t) \in \NN_{\geq 0}$ satisfying $r_1 + \ldots + r_t = r$, let $B(r_1, \ldots, r_t)$ denote the number of bases $B$ such that $|B \cap S_i| = r_i$ for all $1 \leq i \leq t$. Let $\{v_1, \ldots, v_n\}$ be a unimodular coordinatization of $M$ where $v_i \in \RR^r$. Then 
	\[
		B(r_1, \ldots, r_t) = \binom{r}{r_1, \ldots, r_t} \mathsf{D} (A_1 [r_1], \ldots, A_t [r_t])
	\]
	where $A_i = X_i X_i^T$ where $X_i$ is the matrix containing as columns the vectors cooresponding to the vectors in $S_i$. Specializing to the case where $t = 2$, we have that 
	\[
		N_k = \binom{r}{k} \mathsf{D}(A_1[k], A_2[r-k]). 
	\]
	Thus the equality $\widetilde{N}_k^2 = \widetilde{N}_{k-1} \cdot \widetilde{N}_{k+1}$ is realy a question on the equality of 
	\[
		\mathsf{D}(A_1[k], A_2[r-k])^2 \geq \mathsf{D}(A_1[k-1], A_2[r-k+1]) \cdot \mathsf{D}(A_1[k+1], A_2[r-k-1]). 
	\]
\end{proof}
 
\begin{thm}[Alexandroff Inequality for Mixed Discriminants]
	Let $A_1, \ldots, A_{n-r}, A$ be $n \times n$ positive definite matrices. Then, 
	\[
		\mathsf{D}(A_1, \ldots, A_{n-r}, A[r])^2 \geq \mathsf{D} (A_1, \ldots, A_{n-r}, A_{n-r}, A[r-1]) \cdot \mathsf{D} (A_1, \ldots, A_{n-r-1}, A[r+1]), 
	\]
	where equality holds if and only if $A = \alpha A^{n-r}$ for some real $\alpha$. 
\end{thm}

\begin{conj} \label{main-conjecture}
	Let $M$ be a matroid of rank $r$ and order $n$. Then,  
	\begin{enumerate}[label = (\alph*)]
		\item $N_k$ is log-concave.
		\item $N_k /\binom{n}{k}$ is log-concave.
		\item $N_k / \binom{r}{k}$ is log-concave.
	\end{enumerate}
\end{conj}

The conjectures (a), (b), (c) are listed in order of increasing strength. 

\begin{prop}
	In Conjecture~\ref{main-conjecture}, (c) $\implies$ (b) $\implies$ (a). 
\end{prop}

\begin{proof}
	Conjecture (c) asserts that 
	\[
		\frac{N_k^2}{N_{k-1}N_{k+1}} \geq \frac{k+1}{k} \cdot \frac{r-k+1}{r-k}.
	\]
	Conjecture (b) asserts that
	\[
		\frac{N_k^2}{N_{k-1}N_{k+1}} \geq \frac{k+1}{k} \cdot \frac{n-k+1}{n-k}.
	\]
	Conjecture (a) asserts that 
	\[
		\frac{N_k^2}{N_{k-1}N_{k+1}} \geq 1. 
	\]
	The proposition follows from the inequality 
	\[
		\frac{r-k+1}{r-k} \geq \frac{n-k+1}{n-k} \geq 1.
	\]
\end{proof}

\begin{prop}
	In the case for uniform matroids, Conjecture~\ref{main-conjecture}(c) is true. 
\end{prop}

\begin{proof}
	Consider the uniform matroid $U_{r, n}$ and partition $|A| = a, |B| = b$ such that $a + b = n$. Then, for any $k$ we have
	\begin{align*}
		N_k & = \binom{a}{k} \cdot \binom{b}{r-k} \\
		N_{k+1} & = \binom{a}{k+1} \cdot \binom{b}{r-k-1} \\
		N_{k-1} & = \binom{a}{k-1} \cdot \binom{b}{r-k+1}.
	\end{align*}
	We can compute
	\begin{align*}
		\frac{N_k^2}{N_{k-1} N_{k+1}} & = \frac{\binom{a}{k}^2}{\binom{a}{k+1} \binom{a}{k-1}} \cdot \frac{\binom{b}{r-k}^2}{\binom{b}{r-k-1} \binom{b}{r-k+1}} \\
		& = \frac{k+1}{k} \cdot \frac{a-k+1}{a-k} \cdot \frac{r-k+1}{r-k} \cdot \frac{b-r+k+1}{b-r+k} \\
		\frac{\binom{r}{k-1} \binom{r}{k+1}}{\binom{r}{k}^2} & = \frac{k}{k+1} \cdot \frac{r-k}{r-k+1}.
	\end{align*}
	Thus, we have
	\[
		\frac{(N_k / \binom{r}{k})^2}{(N_{k+1} / \binom{r}{k+1}) (N_{k-1} / \binom{r}{k-1})} \geq \frac{a-k+1}{a-k} \cdot \frac{b-r+k+1}{b-r+k} > 1. 
	\]
\end{proof}

Using the recent methods developed in \cite{lorentzian-polynomials}, we can extend Theorem~\ref{stanley-theorem} without the extra regularity condition. 

\begin{thm} \label{general-theorem}
	Let $M$ be a matroid of rank $r$ (not necessarily regular). Then $N_k / \binom{r}{k}$ is log-concave. 
\end{thm}

The proof of Theorem~\ref{general-theorem} uses the Lorentzian property of the basis generating polynomial of a matroid. 

\begin{defn}
	Let $M = (E, \mcB)$ be a matroid with bases $\mathcal{B}$. Then, we define the \textbf{basis generating polynomial} of $M$ to be the polynomial in variables $x = \{x_e\}_{e \in E}$ defined by 
	\[
		F_M(x) = \sum_{B \in \mathcal{B}} x_B
	\]
	where for any subset $S \subseteq E$ we define the monomial
	\[
		x_S := \prod_{s \in S} x_s. 
	\]
\end{defn}

\begin{prop}
	Let $M = (E, \mcB)$ be a matroid. Then the basis generating polynomial $F_M(x)$ is Lorentzian. 
\end{prop}

\begin{proof}
	Using the same notation as in \cite{lorentzian-polynomials}, it suffices to prove that the support of $F_M(x)$ is $M$-convex and all partials are Lorentzian. The first follows from the fact that the support consists of all bases of the matroid. To prove that all partials are Lorentzian, we induct on the size of the matroid. Indeed, since all exponents are $1$ we have 
	\[
		\partial_e F_M(x) = F_{M / \{e\}} (x).
	\]
	This is Lorentzian from the inductive hypothesis since $M / \{e\}$ has one less element. 
\end{proof}

\begin{proof}[Proof of Theorem~\ref{general-theorem}]
	Set all variables indexed by elements in $A$ by $u$ and all variables indexed by elements in $B$ by $v$. Then the resulting polynomial is 
	\[
		\sum_{k = 0}^r N_k u^k v^{r-k}
	\]
	and this is Lorentzian. This implies that the coefficients are ultra-log-concave. This suffices for the proof. 
\end{proof}

\textit{Remark.} Stanley's method of proof would not have worked with non-regular matroids because there exist basis generating polynomials which cannot be written as the mixed volume polynomial for any set of convex bodies.

\section{Equality Case}

\begin{example}
	Let $A = \{e_1, \ldots, e_n\}$ and $B = \{f_1, \ldots, f_n\}$ such that the bases consist of all two element pairs except $\{e_i, f_i\}$ for all $1 \leq i \leq n$. Then $N_0 = \binom{n}{2}$, $N_1 = n(n-1)$, and $N_2 = \binom{n}{2}$. We have that 
	\[
		\frac{N_0}{\binom{2}{0}} = \frac{N_1}{\binom{2}{1}} = \frac{N_2}{\binom{2}{2}} = \binom{n}{2}. 
	\]
\end{example}

We make the following conjecture. 

\begin{conj} \label{new-conjecture}
	For simple matroids, we have strict inequality in the ultra-log-concavity. 
\end{conj}

Using the small matroid database of Gordon Royle and Dillon Mayhew (cite in a more official manner), we have verified the following result. 

\begin{thm}
	Conjecture~\ref{new-conjecture} holds for matroids of size at most $9$. 
\end{thm}

We attempt to prove this conjecture for three different cases. 
\begin{enumerate}[label = (\roman*)]
	\item graphic matroids
	\item unimodular matroids
	\item general matroids 
\end{enumerate}

We can prove the general version of the ultra-log concavity of the matroid counting function. Consider a partition of our matroid $E = E_1 \sqcup E_2 \sqcup \ldots \sqcup E_s$. Let $B(r_1, \ldots, r_s)$ be the number of bases of our matroid that have $r_i$ elements in $E_i$. 

\begin{thm}
	Let $M = (E, \mathfrak{I})$ be a unimodular matroid of rank $n$, and let $r_1, \ldots, r_s$ be non-negative integers adding to $n$. If $r_{s-1} \geq 1$, $r_s \geq 1$, then 
	\[
		B(r_1, \ldots, r_s)^2 \geq B(r_1, \ldots, r_{s-2}, r_{s-1} + 1, r_s - 1) \cdot B(r_1, \ldots, r_{s-2}, r_{s-1} - 1, r_{s} + 1)
	\]
\end{thm}

\begin{proof}
	Let $X$ be the $n \times |E|$ unimodular matrix representing $M$. Partition $X$ into $X = \begin{bmatrix} X_1 & \ldots & X_s \end{bmatrix}$ where $X_i$ contains the columns corresponding to the elements in $E_i$. Since our matrix is unimodular, we have 
	\[
		B(r_1, \ldots, r_s) = \frac{1}{r_1! \ldots r_s!} \cdot \sum \det (x_1, \ldots, x_n)^2
	\]
	where in the sum the first $r_1$ entries are columns in $X_1$, the next $r_2$ are columns in $X_2$, and so on. Let $A_k = X_k X_k^T$. From ..., we have 
	\[
		D(A_1 [r_1], \ldots, A_s [r_s]) = \frac{1}{n!} \sum \det (x_1, \ldots, x_n)^2 = \frac{B(r_1, \ldots, r_s)}{\binom{n}{r_1, \ldots, r_s}}. 
	\] 
	The result now follows from the Alexandrov inequality for mixed discriminants. 
\end{proof}

Let's consider the case $s = 2$, as we have been doing. In this case, we have a partition $E = A \sqcup B$, and $N_k = D(A_1[k], A_2[n-k])$ where $A_1 = X_1 X_1^T$ and $A_2 = X_2 X_2^T$ where $X_1$ is the part of the unimodular matrix corresponding to the elements in $A$ and $X_2$ is the part of the unimodular matrix corresponding to the elements in $B$. Let $e_1, \ldots, e_n \in \RR^a$ be the rows of $X_1$ and $f_1, \ldots, f_n \in \RR^b$ be the rows of $X_2$. Then $A_1 = [\langle e_i, e_j \rangle ]$ and $A_2 = [\langle f_i, f_j \rangle]$. In the equality case, we must have $\langle e_i, e_j \rangle = \lambda \langle f_i, f_j \rangle$. 

\begin{thm}
	Suppose that $\frac{N_k}{\binom{r}{k}}^2 = \frac{N_{k-1}}{\binom{r}{k-1}} \frac{N_{k+1}}{\binom{r}{k+1}}$ for some $k$. Then $N_i = \lambda^r c \binom{r}{i}$ for all $0 \leq i \leq r$ where $\lambda \in \QQ_{> 0}$ and $c \in \ZZ_{> 0}$. 
\end{thm}

\begin{proof}
	Let $D_i = D(A_1[i], A_2 [r-i]) = N_i / \binom{r}{i}$. From the equality case of the Alexandrov inequality for mixed discriminants, we have $A_1 = \lambda A_2$ for some real $\lambda$. But then it's an equality case for all things. In particular, we have 
	\[
		\frac{D_1}{D_0} = \frac{D_2}{D_1} = \ldots = \frac{D_r}{D_{r-1}} = \lambda
	\]
	for some $\lambda \geq 0$. Thus, for any $k \geq 1$, we have 
	\[
		\frac{D_k}{D_0} = \prod_{i = 1}^k \frac{D_{i}}{D_{i-1}} = \lambda^r \implies D_k = \lambda^r D_0
	\]
\end{proof}

In particular, for equality to hold if suffices to verify that $N_0 / \binom{r}{0} = N_1 / \binom{r}{1}$ or $N_1 = r N_0$. 

\begin{thm}
	If equality holds at some index, then $N_k = a\binom{r}{k}$ for some $ a \geq 0$ and $c > 0$. 
\end{thm}

\begin{proof}
	From the previous theorem, we know that 
	\[
		\lambda = \frac{N_0}{\binom{r}{0}} = \ldots = \frac{N_r}{\binom{r}{r}} \implies N_k = \binom{r}{k} \lambda 
	\]
\end{proof}
This proves that, at least for unimodular matrices, we have $N_k = \lambda \cdot c^k \binom{r}{k}$ in the equality case. We now prove the conjecture for graphic matroids. 

\begin{thm}
	Suppose that $M$ is a graphic matroid corresponding to a connected graph. Then Conjecture~\ref{new-conjecture} is true.
\end{thm}
\begin{proof}
	  Let $B$ be the incidence matrix. Then it is a totally unimodular matrix which represents $M$. We can split it up into the columns in $A$ and $B$ say $B = [B_1 | B_2]$. Let $C$ be the reduced incidence matrix which comes from $B$ with the last row removed. Then $C = [X_1 | X_2]$ where $B_1 = \begin{bmatrix} X_1 \\ v_1^T \end{bmatrix}$ and $B_2 = \begin{bmatrix} X_2 \\ v_2^T \end{bmatrix}$ where $v_1, v_2 \in \{-1, 0, 1\}^e$. The equality case is $X_1X_1^T = X_2X_2^T$. Note that $B_1B_1^T$ and $B_2B_2^T$ are the Laplacian matrix when only considering edges in $A$ and $B$, respectively. Moreover, $X_1X_1^T$ and $X_2X_2^T$ is the top $r \times r$ matrix of the $r+1$ matrix of $B_1B_1^T$ and $B_2B_2^T$. The dimensions are like this because the graph is connected. In the case where our graph is simple, note that an entry in $X_1X_1^T$ is $\pm$ if and only if an entry in $X_2X_2^T$ is $\pm$. These matrices are the Laplacians corresponding to the graph on the first $r$ vertices and on the edges in $A$ and $B$, respectively except the diagonal terms might be different. Thus, $v_i, v_j$ for $i, j \leq r$ are not adjacent in $A$ if and only if it is not adjacent in $B$. Thus, if has an edge in $A$, then there must be a parallel edge in $B$. This cannot be the case. Hence there cannot be any edges here. Thus all edges must be connected to $v_{r+1}$. So, it suffices to consider the conjecture on the claw graph on $r+1$ vertices. The corresponding matroid is the boolean matroid. In this case, the only basis is the whole graph, so equality does not hold. This suffices for the proof. 
\end{proof}

\section{Current Progress}

Our problem can be reframed in the following way. Given a matroid $M$ on $[n]$, we can define its \textbf{basis generating polynomial} as 
\[
	f_M (x_1, \ldots, x_n) := \sum_{B \in \mathsf{BASE}(M)} \left ( \prod_{i \in B} x_i \right ). 
\]
We can then define the polynomial
\[
	g_M (x_A, x_B) := f_M(x_A, \ldots, x_A, x_B, \ldots, x_B) = \sum_{k = 0}^r N_k x_A^k x_B^{r-k}.
\]
From \cite{MNY} we have the following technical lemma
\begin{lem}[Lemma 4.2 in \cite{MNY}]
	Let $f \in \RR[x_1, \ldots, x_n]$ be a homogeneous polynomial having the $\text{HRR}_1$ with respect to $l_a$ with $a \in \RR^n$ and let $l_1, l_2 \in S$ be linear forms. If $l_1$ and $l_2$ are $\RR$-linearly independent in $R_f^1$ and $(l_1l_2f)(a) > 0$, then 
	\[
		\det \begin{pmatrix}
			(l_1l_1f)(a) & (l_1l_2f)(a) \\
			(l_1l_2f)(a) & (l_2l_2f(a)
		\end{pmatrix} < 0.
	\]
\end{lem}

It seems like it would be enough to prove the following things:
\begin{itemize}
	\item $g(x_A, x_B)$ has $\text{HRR}_1$ wrt $\partial_A$
	\item $\partial_A$ and $\partial_B$ are linearly independent in $R_f^1$
	\item $\partial_A^2 g (1, 0) > 0$. 
\end{itemize}

Given a matroid $M$, define its truncation $TM$ to be the matroid on the same set with bases being the independent sets of rank $\text{rank}(M) - 1$. 

\begin{prop}
	Let $M$ be a matroid. Then, we have that 
	\[
		g_{TM} (x_A, x_B) = \sum_{k = 0}^{r-1} \left \{ (k+1)N_{k+1} + (r-k) N_k\right \} x_A^k x_B^{r-k-1} = (\partial_A + \partial_B) g_M
	\]
\end{prop}

\begin{proof}
	Combinatorially the formula is clear, but if that is not enough we have that 
	\begin{align*}
		g_{TM}(x_A, x_B) & = f_{TM}(x_A, \ldots, x_A, x_B, \ldots, x_B) \\
		f_{TM} & = (\partial_1 + \ldots + \partial_n) f_M \\
		& = (\partial_1 + \ldots + \partial_n) \sum_{k = 0}^r \sum_{U \in \mathcal{B}(M): |U \cap A| = k} x^U \\
		& = \sum_{k = 0}^r \sum_{|U \cap A| = k} (\partial_1 + \ldots + \partial_n) x^U. 
	\end{align*}
	Substituting $x_A$ and $x_B$, we get 
	\begin{align*}
		g_{TM}(x_A, x_B) & = \sum_{k = 0}^r N_k (kx_A^{k-1} x_B^{r-k} + (r-k)x_A^k x_B^{r-k-1}) \\
		& = \sum_{k = 1}^r k N_k x_A^{k-1} x_B^{r-k} + \sum_{k = 0}^{r-1} (r-k)N_k x_A^k x_B^{r-k-1} \\
		& = \sum_{k = 0}^{r-1} (k+1)N_{k+1} x_A^k x_B^{r-k-1} + \sum_{k =0}^{r-1} (r-k) N_k x_A^k x_B^{r-k-1} \\
		& = \sum_{k = 0}^{r-1} \left \{ (k+1)N_{k+1} + (r-k) N_k\right \} x_A^k x_B^{r-k-1}.
	\end{align*}
	This seems less than desirable.
\end{proof}
Perhaps there is another operator to make matroids smaller that preserves simplicity? Or maybe the truncation operator is good enough to imply the desired inequalities. 

\section{Hodge-Riemann Relations for the Basis Generating Polynomial}

For every homogeneous polynomial in real coefficients, we can associate a ``cohomology ring'' which satisfy nice properties. 

\begin{defn}
	Let $f \in \RR[x_1, \ldots, x_n]$ be a homogeneous polynomial and let $S := \RR [ \partial_1, \ldots, \partial_n]$ be the polynomial ring in variables which are differentials corresponding to $x_1, \ldots, x_n$. We define 
	\[
		A^\bullet_f := S / \text{Ann}(f)
	\]
	where we give it a graded ring structure graded by degree. 
\end{defn}

The cohomology ring described above is a Poincare duality algebra. Explicitly, this means that if $f$ is a homogeneous polynomial of degree $d$, then $A^d_f \simeq \RR$ and the bilinear map $A_f^k \times A_f^{d-k} \to A_f^d \simeq \RR$ is non-degenerate. 

\begin{prop}
	Let $f \in \RR[x_1, \ldots, x_n]$ be a homogeneous polynomial of degree $d$. Then $A_f^\bullet$ is a Poincare Duality algebra.
\end{prop}

\begin{proof}
	Consider the map $A_f^d \to \RR$ by $\xi \mapsto \xi \cdot f$. This is clearly surjective. This is injective because $\xi \cdot f = 0$ if and only if $\xi \in \text{Ann}(f)$. Now, we prove that $A_f^k \times A_f^{d-k} \to \RR$ is non-degenerate. Suppose that $\xi \alpha \cdot f = 0$ for all $\alpha \in A_f^{d-k}$. This implies that $\alpha (\xi f) = 0$ for all $\alpha \in A_f^{d-k}$. If $\xi f \neq 0$, then we can take $\alpha$ to be one of the dual differential operators to a non-zero monomial. This will get a non-zero value and is a contradiction. 
\end{proof}

\begin{defn} [Strong Lefschetz Property]
	Let $l \in S$ be a linear form. We say that a homogeneous polynomial $f \in \RR[x_1, \ldots, x_n]$ satisfies the Strong Lefschetz Property of degree $k$ with respect to $l$ (also notated $\mathsf{SLP}_k$ with respect to $l$) if the map 
	\[
		\times l^{d-2k} : A_f^k \to A_f^{d-k}
	\]
	is an isomorphism. 
\end{defn}

\begin{defn} [Hodge-Riemann Relation]
	We say $f$ satisfies the Hodge-Riemann Relation of degree $k$ with respect to $l$ (also notated as $\mathsf{HRR}_k$ with respect to $l$) if it satisfies $\mathsf{SLP}_k$ with respect to $l$ and the bilinear form 
	\[
		Q^k_l : A_f^k \times A_f^k \to \RR
	\] 
	defined by $Q^k_l(\xi_1, \xi_2) = (-1)^k \xi_1 \xi_2 l^{d-2k}$ is positive definite on the kernel of $\times l^{d-2k+1} : A_f^k \to A_f^{d-k+1}$.
\end{defn}

\begin{prop}[Non-degeneracy] \label{non-degeneracy}
	Let $B : V \times W \to k$ be a bilinear pairing between two finite-dimensional $k$-vector spaces $V$ and $W$. Two of the following imply the third.
	\begin{enumerate}[label = (\roman*)]
		\item The map $B_V : V \to W^*$ defined by $v \mapsto B(v, \cdot)$ has trivial kernel. 

		\item The map $B_W : W \to V^*$ defined by $w \mapsto B(\cdot, w)$ has trivial kernel. 

		\item $\dim V = \dim W$.
	\end{enumerate} 
\end{prop}

We say a bilinear pair is non-degenerate if it satisfies all of the conditions in Proposition~\ref{non-degeneracy}. 
\begin{proof}
	Condition (i) implies $\dim V \leq \dim W$ and condition (ii) implies $\dim W \leq \dim V$. Thus (i) and (ii) imply (iii). Now suppose that (i) and (iii) are true. Then $B_V$ is an isomorphism between $V$ and $W^*$. Let $v_1, \ldots, v_n$ be a basis for $V$. Then $B_V(v_1), \ldots, B_V(v_n)$ is a basis of $W^*$. Then $w_1, \ldots, w_n$ be the dual basis in $W$ with respect to this basis of $W^*$. Suppose that $\sum \lambda_i w_i \in \ker B_W$. Then for all $v \in V$, we have
	\[
		\sum_{i = 1}^n \lambda_i B_V(v)(w_i) = B \left (v, \sum_{i = 1}^n \lambda_i w_i \right ) = 0.
	\]
	By letting $v = v_1, \ldots, v_n$ we get $\lambda_i = 0$ for all $i$. 
\end{proof}

\begin{lem} [Lemma 3.4 in \cite{MNY}]
	Let $f \in \RR [x_1, \ldots, x_n]$ be a homomogeneous polynomial of degree $d$ and $a \in \RR^n$ be a point such that $f(a) > 0$. Then for $l= l_a$, we have
	\begin{enumerate}[label = (\alph*)]
		\item $f$ satisfies $\mathsf{SLP}_1$ with respect to $l$ if and only if $Q_l^1$ is non-singular. 
		\item $f$ satisfies $\mathsf{HRR}_1$ with respect to $l$ if and only if $-Q_l^1$ has signature $(+, -, \ldots, -)$. 
	\end{enumerate}
\end{lem}

\begin{proof}
	We include the proof. Let $m_f^1 : A_f^1 \times A_f^{d-1} \to A_f^d$ be the multiplication map. Then $Q_l^1 (\xi_1, \xi_2) = - m_f^1(\xi_1, \xi_2 l^{d-2})$. 
	\[\begin{tikzcd}
	{A_f^1 \times A_f^1} && {A_f^1 \times A_f^{d-1}} \\
	\\
	{A_f^d}
	\arrow["{Q_l^1}"', from=1-1, to=3-1]
	\arrow["{(\xi_1, \xi_2) \mapsto (\xi_1, - \xi_2 l^{d-2})}", from=1-1, to=1-3]
	\arrow["{m_f^1}", from=1-3, to=3-1]
\end{tikzcd}\]

\[\begin{tikzcd}
	{A_f^1} && {A_f^{d-1}} \\
	\\
	{(A_f^1)^*}
	\arrow["{(Q_l^1)_{A_f^1}}", from=1-1, to=3-1]
	\arrow["{\mathsf{PD}_1}", from=1-1, to=1-3]
	\arrow["{-(m_f^1)_{A_f^{d-1}}}", from=1-3, to=3-1]
\end{tikzcd}\]

These diagrams prove (a). This should actually be true for any degree $k$. Now for (b), let $\psi_a : A_f^1 \to A_f^d$ by $\psi_a(h) = l_a^{d-1}h$. Then the map $A_f^0 \to A_f^1 \to A_f^d$ is an isomorphism. Thus, we know $R_f^1 = \RR l_a \oplus \ker \psi_a$ and the two subspaces are orthogonal with respect to $Q_l^1$. We have 
\[
	Q_{l_a}^1(l_a, l_a) = - l_a^d (f) = d! f(a) > 0.
\]
Thus, we know that $A_f$ satisfies $HRR_1$ if and only if $-Q_{l_a}^1$ has signature $(+, -, \ldots, -)$. 
\end{proof}

\begin{lem} [Lemma 3.5 in \cite{MNY}]
	If $f$ is Lorentzian and $f(a) > 0$, then $f$ has $\mathsf{HRR}_1$ if and only if it has $\mathsf{SLP}_1$. 
\end{lem}

\begin{proof}
	$H_f|_a$ is the matrix representing $Q_l^1$ with respect to the generating set $\partial_1, \ldots, \partial_n$. Thus $Q_l^1$ has at most one positive eigenvalue. There is at least one positive eigenvalue since $f(a) > 0$ and $Q_l^1(l, l) = d!f(a)$. This completes the proof. 
\end{proof}

\begin{lem}
	If $f$ is Lorentzian, then $\mathsf{HRR}_1$ holds on $\RR_{> 0}^n$. 
\end{lem}

One possible question to ask about the basis generating polynomial $f_M$ is whether or not it satisfies Hodge-Riemann Relation on the facets of the positive orthant. We can decompose the boundary of the positive orthant as 
\[
	\partial \RR_{\geq 0}^E = \bigcup_{i \in E} H_i
\]
where $H_i := \{x \in \RR_{\geq 0}^n : \langle e_i, x \rangle = 0\}$. We say $f$ satisfies $\mathsf{SLP}$ or $\mathsf{HRR}$ with respect to $a \in \RR^n$ when it satisfies the property with respect to the linear differential operator
\[
	l_a := a_1 \partial_1 + \ldots + a_n \partial_n. 
\]

\begin{conj} \label{HRR-conjecture}
	Let $M$ be a simple matroid. The basis generating polynomial $f_M$ satisfies $\mathsf{HRR}_1$ for $f_M$ on $\text{relint}(H_i)$ if and only if $i$ is not a coloop in $M$. 
\end{conj}

It is easy to see that if $i$ is a coloop, then $\mathsf{HRR}_1$ is not satisfied on the corresponding facet. Indeed, the bilinear form $-Q^1_l$ will have the same signature has the Hessian. The Hessian would be singular in the case that there is a coloop. If we don't require the matroid to be simple, consider the matroid $M = ([3], B)$ where $B = (\{1, 3\}, \{2, 3\})$. Then $e = 1$ is not a coloop, but since in the simplification it is a coloop it doesn't satisfy the conjecture. This implies that the restriction that $M$ is simple is necessary. 

\begin{thm}
 	Conjecture~\ref{HRR-conjecture} is true for rank $2$ simple matroids. 
\end{thm}
\begin{proof}
	In this case, we have $f_M = \sum_{i < j} x_i x_j$. It suffices to compute the signature of the Hessian. The Hessian is independnet of the choice of $a$ and is equal to $a_{ij} = \1_{i \neq j}$. From Proposition 1.5 of \cite{Stanley-alg-combo}, this has eigenvalues $-1$ with multiplicity $n-1$ and $n-1$ with multiplicity $1$. Thus it has signature $(+, -, \ldots, -)$ and the conjecture is true for this case. 
\end{proof}
\begin{fact}
	Let $e \in M$ be an element of the matroid that is not a co-loop. Then 
	\[
		f_M = x_e f_{M / e} + f_{M \backslash e}.
	\]
\end{fact}
\begin{proof}
	We omit the proof. 
\end{proof}

If $M$ is simple, then the matroid $M \backslash e$ is still simple. 

\begin{fact}
	The cohomology ring of $f_M$ is isomorphic to the cohomology ring of $f_{\widetilde{M}}$ where $\widetilde{M}$ is the simplification of $M$. 
\end{fact}

\begin{proof}
	We omit the proof. 
\end{proof}


\begin{prop}
	$\partial_1 f_M, \ldots, \partial_n f_M$ are linearly independent if and only if $M$ is simple.  
\end{prop}
\begin{proof}
	If $M$ is not simple, it either has loops or parallel elements. If $i$ is a loop, then $\partial_i f_M = 0$. If $i, j$ are parallel, then $(\partial_i - \partial_j) f_M = 0$. This suffices for the proof. 
\end{proof}

\subsection{Current Approach}

For any $a \in \RR^n$ with $f_M(a) > 0$, we know that $f_M$ has $\HRR_1$ if and only if it has $\SLP_1$. Let us consider $a = (a_1, \ldots, a_{n-1}, 0)$ or even $a_1 = \ldots = a_{n-1} = 1$ and $a_n = 0$. For $i, j \in [n-1]$, we have 
\[
	\partial_i \partial_j f = x_n \partial_i \partial_j f_{M / n} + \partial_{i} \partial_j f_{M \backslash n}.
\]
When $a \in H_n$, we have $\partial_i \partial_j f = \partial_i \partial_j f_{M \backslash n}$.  If $j = n$, then we have 
\[
	\partial_i \partial_n f = \partial_i f_{M / n}.
\]
Thus, the Hessian of $f$ at $a$ where $a_n = 0$ is of the form 
\[
	H_f = \begin{bmatrix}
		H_{f_{M \backslash n}} & \nabla f_{M/n} \\
		\left ( \nabla f_{M/n} \right )^{\mathsf{T}} & 0
	\end{bmatrix}.
\]
We want to know when this matrix is non-singular. Luckily, we have a neat formula to compute block determinants. 

\begin{thm} [\cite{Silvester2000DeterminantsOB}]
	When $D$ is invertible and $A$ is a square matrix, we have
	\[
		\det \begin{bmatrix}
			A & B \\ C & D
		\end{bmatrix} = \det (A - BD^{-1}C) \det (D).
	\]
\end{thm}

This allows us to compute
\[
	\det H_f = \det H_{f_{M \backslash n}} \cdot (\nabla f_{M / n})^T H_{f_{M\backslash n}}^{-1} \cdot \nabla f_{M/n}. 
\]
We know that $\det H_{f_{M\backslash n}} \neq 0$ since $M \backslash n$ is simple and we are substituting an element from the positive orthant into it. So, we want to prove that 
\[
 	\nabla f_{M/n}^\mathsf{T} \cdot H_{f_{M - n}}^{-1} \cdot \nabla f_{M/n} \neq 0.
\]

For example, this proves that for $M = U_{n, k}$ uniform matroid with $l = \partial_1 + \ldots + \partial_{n-1}$, we know that $f_M$ has $\mathsf{HRR}_1$ with respect to $l$. 

\begin{thm}
	Let $M$ be a matroid of rank $r \geq 3$ and $f_M$ its basis generating polynomial. Then 
	\[
		\{\xi \in A_f^1 : \partial_i \xi = 0 \text{ for $i = 2, \ldots, n$}\} = \{0\}.
	\]
	In other words, if $\xi = \sum_{i = 1}^n c_i \partial_i$ for real constants $c_i \in \RR$, and $\xi (\partial_i f_M) = 0$ for $i = 2, \ldots, n$, then this implies that $\xi f_M = 0$. 
\end{thm}

\begin{proof}
	For all $i \in [n]$, we have that
	\begin{align*}
		\xi \partial_i f_M & = \sum_{j \neq i} c_j \partial_i \partial_j f_M \\
		& = \sum_{j = 1}^n c_j \sum_{\substack{\alpha \in \mathcal{I}_{r-2} \\ \alpha \cup \{i,j\} \in \mathcal{I}_r}} x^\alpha \\
		& = \sum_{\substack{\alpha \in \mathcal{I}_{r-2} \\ \alpha \cup \{i\} \in \mathcal{I}_{r-1}}}\left ( \sum_{\substack{j \in [n] \\ \alpha \cup \{i, j\} \in \mathcal{I}_r}} c_j \right ) x^\alpha.
	\end{align*}
	When $i \neq 1$, this expression is $0$. Thus, for all $i \neq 1$ and $\alpha \in \mathcal{I}_{r-2}$ such that $\alpha \cup \{i\} \in \mathcal{I}_{r-1}$, we have that 
	\[
		\sum_{j \in [n] : (\alpha \cup \{i\}) \cup \{j\} \in \mathcal{I}_r} c_j = \sum_{j \in [n] : \alpha \cup \{i, j\} \in \mathcal{I}_r} c_j = 0.
	\]
	Since $r \geq 3$, any $\beta \in \mathcal{I}_{r-1}$ can be written as $\beta = \alpha \cup \{i\}$ where $i \notin \alpha$ and $i \neq 1$. Thus, for all $\beta \in \mathcal{I}_{r-1}$ we have
	\[
		\sum_{j \in [n] : \beta \cup \{j\} \in \mathcal{I}_r} c_j = 0.
	\]
	But then,
	\begin{align*}
		\xi f = \sum_{j = 1}^n c_j \partial_j f_M & = \sum_{j = 1}^n c_j \sum_{\substack{\beta \in \mathcal{I}_{r-1} \\ \beta \cup \{j\} \in \mathcal{I}_r}} x^\beta \\
		& = \sum_{\beta \in \mathcal{I}_{r-1}} \left ( \sum_{\substack{j \in [n] \\ \beta \cup \{j\} \in \mathcal{I}_r}} c_j \right ) x^\beta = 0.
	\end{align*}
	This suffices for the proof. 

\end{proof}
\bibliographystyle{plain}
\bibliography{ref}


\end{document}


